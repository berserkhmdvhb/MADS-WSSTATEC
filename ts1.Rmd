---
title: "Daily Climate Time Series Analysis"
author: "Hamed Vaheb"
#1date: "09 Jan 2021"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---
# **Import Libraries**

```{r setup, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr) # for knitting markdown files
library(dplyr)
library(ggplot2) # for plotting
library(broom)
library(reshape2)
#library(readr)
#library(readxl)
#library(Ecdat)
library(janitor)
#library(plm)
#library(pwt9)
#library(quarto)
library(renv)
library(shiny)
library(targets)
library(testthat)
library(tidyverse)
library(tibble)
library(lubridate)
library(purrr)
library(Hmisc) # for dependence tests
library(plotly) # for interactive plots
library(hrbrthemes) 
library(xts) # for time series objects
#library(zoo)
library(seasonal) # for seasonality of time series
library(tsbox)
library(forecast) # for forecasting time series
library(tseries) # for unit root tests
#library(seasonalview)
#library(autoplotly)

library(tidyverse) # general
#library(ggalt) # dumbbell plots
library(plotly) #for drawing interactive plots
library(ggridges) #for drawing density gradient
library(shades) #edit colors in natural ways:
library(urca) 
library(tseries)
library(vars) # for VAR models
library(dynlm)
library(Metrics)
library(htmlTable) # for showing tables
#library(keras)
#library(tensorflow)
#install_keras()
#install_tensorflow(version = "nightly")


```

```{r}
library(reticulate)
Sys.unsetenv("RETICULATE_PYTHON") 
use_virtualenv("~/.virtualenvs/r-reticulate")
library(keras)
```

```{r}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```
```{r}
#webshot::install_phantomjs()
```


# **Introduction**

## **Describe Dataset**

The dataset used for this project is [Daily Delhi Climate](https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data), which consists of the following columns:

1. `date`: Date of format YYYY-MM-DD starting from "2013-01-01" and ending in "2017-01-01".
<br/>
2. `meantemp`: Mean temperature averaged out from multiple 3 hour intervals in a day.
<br/>
3. `humidity`: Humidity value for the day (units are grams of water vapor per cubic meter volume of air).
<br/>
4. `wind_speed`: Wind speed measured in kmph.
<br/>
5. `mean_pressure`: Pressure reading of weather (measure in atm)


## **Goal and Procedure**

The goal of this project is to analyze and forecast the mean temperature of Delhi, which is recorded in the `meantemp` column. For some models, the wind speed of Delhi is also predicted.

The following four forecasting models are used for this work:
autoregressiveâ€“moving-average model (ARMA), vector autoregression (VAR), feedforward neural network (NN), and long-short term memory (LSTM) neural network.
After importing the dataset, outliers are removed in [Preprocessing and Analysis Section](#preprocess) section. Then `meantemp` column is assigned to a time series object in [Construct Time Series](#initts) section for further processing.
After detecting seasonalities using [plots](#seasplots), the time series is [seasonally adjusted](#deseas) using [X13-ARIMA-SEATS](https://cran.r-project.org/package=seasonal/vignettes/seas.pdf) decomposition model. Then, remaining trend is removed in [detrend](#detrend).

Before forecasting the time series, I check for staionarity of time series, as stationarity is an assumption in ARIMA model.
For this purpose, unit root tests are applied in [Stationarity](#stationary) section.

Finally, I used ARIMA model to forecast the time series in [Forecast Time Series](#forecast) section.

```{r}

df_train <- read_csv("data/DailyDelhiClimateTrain.csv")
df_test <- read_csv("data/DailyDelhiClimateTest.csv")
```

```{r}
summary(df_train)
```
```{r}
df_train |> dplyr::glimpse()
```
```{r}
df_test |> summary()
```

```{r}
df_test |> dplyr::glimpse()
```

# **Visualize Data** {#viz}

Below we can see interactive plot of the time series.

```{r}
p <- df_train |>
  ggplot( aes(x=date, y=meantemp)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("bitcoin price ($)") +
    theme_ipsum()

# Turn it interactive with ggplotly
p <- ggplotly(p)
#p
p

```

# **Preprocessing** {#preprocess}

## **Treat Outliers: Manual** {#outlilermanual}

Evidenced by the plot of time series, an outlier at the last observation (last row of dataframe) can be detected. It causes an abrupt decrease in value of temperature. This would lead to problems in further analysis and forecasting models. Therefore, last observation's value is replaced with its previous one. In [Treat Outliers: Automatic](#outlierauto), an automatic procedure of treating outliers is performed.


```{r}
previous_value <- df_train$meantemp[df_train$date == as.Date('2016-12-31')]

df_train$meantemp[df_train$date == as.Date('2017-01-01')]<- previous_value 
```

```{r}
#df_train <- head(df_train, -1)
head(df_train)
```

```{r}
tail(df_train)

```

The plot of data after removing the outlier is visualized below:

```{r}
p <- df_train |>
  ggplot( aes(x=date, y=meantemp)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("bitcoin price ($)") +
    theme_ipsum()

# Turn it interactive with ggplotly
p <- ggplotly(p)
#p
p

```


In below we find if there is any missing date
```{r}
date_range <- seq(min(df_train$date), max(df_train$date), by = 1) 
date_range[!date_range %in% df_train$date] 

```
## **Construct Time Series** {#initts}

Thee `meantemp` column is used to create time series data.
The time series is assigned to `xts` objects. But since many functions later require `ts` object, each time a `xts` object is defined, a `ts` counterpart is created for further possible use. The conversion is performed using  `txbox::ts_ts` function.


```{r}
min(df_train$date)
max(df_train$date)
```

```{r}
#ts_train <- zoo(df_train$meantemp, df_train$date)

xts_train_meantemp <- xts(df_train$meantemp, order.by=df_train$date, "%Y-%m-%d")
class(xts_train_meantemp)
head(xts_train_meantemp)
tail(xts_train_meantemp)

# convert xts to ts

## Create a daily Date object for ts
#inds <- seq(as.Date("2013-01-01"), as.Date("2017-01-01"), by = "day")

#set.seed(25)
#ts_train <- ts(df_train$meantemp,     # random data
#           start = c(2013, as.numeric(format(inds[1], "%j"))),
#           frequency = 365)


#ts_train <- ts(df_train$meantemp, start = decimal_date(ymd("2013-01-01")), frequency = 365.25 / 7)


```

Convert XTS objects to TS objects:

```{r}
ts_train_meantemp <- ts_ts(xts_train_meantemp)
head(ts_train_meantemp)
tail(ts_train_meantemp)
```

Static plot is show in the following:

```{r}
ts_plot(xts_train_meantemp)
```


## **Treat Outliers: Automatic** {#outlilerauto}

The [`tsclean`](https://www.rdocumentation.org/packages/forecast/versions/8.3/topics/tsclean) function  identifies and replace outliers and missing values in a time series. It uses Friedman's Super Smoother estimator for non-seasonal series and a robust STL decomposition for seasonal series. To estimate missing values and outlier replacements, linear interpolation is used on the (possibly seasonally adjusted) series. In our case, the robust STL decomposition is used due to existing of seasonalities.



```{r}
ts_plot(xts_train_meantemp)
```

```{r}
xts_train_meantemp <- tsclean(xts_train_meantemp)
```

```{r}
ts_plot(xts_train_meantemp)
```

I speculate that since there was not high fluctuations in the time series, there was no change.
However, for instance in the `wind_speed` column, there are salient changes when using `tsclean`, as evident when applying in [VAR](#var).

## **Seasonality** {#seas}

From the initial plot I judge that there is seasonality. 
For more delicate observation to find if there is more granular periods of seasonality, I use seasonality plots.
Before that, I aggregate data weekly, monthly, and quarterly.

### Seasonality Plots {#seasplots}

```{r}
# Weekly mean temperature
xts_week_train_meantemp <- apply.weekly(xts_train_meantemp,sum)
ts_week_train_meantemp <-ts_ts(xts_week_train_meantemp)

# Monthly mean temperature
xts_mon_train_meantemp <- aggregate(xts_train_meantemp, by=as.yearmon, FUN=sum)
ts_mon_train_meantemp <-ts_ts(xts_mon_train_meantemp)

# Quarterly mean temperature
xts_quar_train_meantemp <- aggregate(xts_train_meantemp, as.yearqtr, FUN=sum)
ts_quar_train_meantemp <-ts_ts(xts_quar_train_meantemp)


# Yearly mean temperate
as.year <- function(x) as.integer(as.yearmon(x))
xts_year_train_meantemp <- aggregate(xts_train_meantemp, by=as.year, FUN=sum)
#ts_year_train_meantemp <-ts_ts(xts_year_train_meantemp)
#xts_year_train_meantemp[1]

```


The year 2017 has only one observation, so it is removed it from all the aggregated datasets. I couldn't do it before aggregating, otherwise I would have confronted the error `Error: series has no regular pattern`.

```{r}

xts_week_train_meantemp <- head(xts_week_train_meantemp, -1)
xts_mon_train_meantemp <- head(xts_mon_train_meantemp, -1)
xts_quar_train_meantemp <- head(xts_quar_train_meantemp, -1)

ts_week_train_meantemp <- head(ts_week_train_meantemp, -1)
ts_mon_train_meantemp <- head(ts_mon_train_meantemp, -1)
ts_quar_train_meantemp <- head(ts_quar_train_meantemp, -1)


```


```{r}
#options(repr.plot.width = 7, repr.plot.height =20)
forecast::ggseasonplot(ts_mon_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1) +
  ylab("degree") +
  ggtitle("Seasonal plot: Monthly Mean Temperature")
```
```{r}
forecast::ggseasonplot(ts_mon_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1, polar=TRUE) +
  ylab("degree") +
  ggtitle("Polar Seasonal plot: Monthly Mean Temperature")
```


```{r}
#options(repr.plot.width = 7, repr.plot.height =20)
forecast::ggseasonplot(ts_quar_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1) +
  ylab("degree") +
  ggtitle("Seasonal plot: Quarterly Mean Temperature")
```

```{r}
forecast::ggseasonplot(ts_quar_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1, polar=TRUE) +
  ylab("degree") +
  ggtitle("Polar Seasonal plot: Quarterly Mean Temperature")
```


Judging from the plots, all months seem to have seasonalities, with stronger ones in second and third quarters of each year.

### Deseasonalize {#deseas}

If one intends to remove different periods of seasonality together, he can use the `forecast::msts` function.
For instance in below, weekly and yearly seasonality are removed together.

```{r}
des_ts_train_meantemp <- msts(xts_train_meantemp,seasonal.periods = c(7,365))
#head(des_xts_train)
#library(tsbox)
#ts_train <-ts_ts(xts_train)
#ts_train

class(des_ts_train_meantemp)

```
The output of `msts` had an a peculiar shape, and it is not using the state-of-the-art X13 decomposition. To address these limitations, I incorporated the [X-13ARIMA-SEATS](http://www.seasonal.website/seasonal.html) using `seasonal:seas` function. it has some of its own limitations, as stated in the package's [reference manua](https://www2.census.gov/software/x-13arima-seats/x-13-data/documentation/docx13as.pdf). For instance, the number of observations must not exceed 780, nor should maximum seasonal period exceed 12.
That is why I couldn't use original data `ts_train` and also the weekly aggregated data `ts_week_train`, as I would confront the error `Seasonal period too large`. The only possible aggregated data with highest frequency possible was monthly aggregated, `ts_mon_train`.
However, I am concerned that I would lose significant pattern and information with this amount of aggregation.

```{r}
length(xts_train_meantemp)
length(ts_train_meantemp)
```



```{r}

length(xts_train_meantemp)
nowXTS <-ts_xts(ts_train_meantemp)
length(nowXTS)

length(ts_week_train_meantemp)
```
```{r}
plot(ts_week_train_meantemp)
length(ts_week_train_meantemp)

```

```{r}
plot(ts_train_meantemp)
length(ts_train_meantemp)

```


```{r}
plot(ts_mon_train_meantemp)
length(ts_mon_train_meantemp)
```


```{r}
m <- seas(ts_mon_train_meantemp)
ts_train_adj_meantemp <- final(m)
#ts_train_adj
length(ts_train_adj_meantemp)
```




```{r}
m <- seas(ts_mon_train_meantemp)
ts_train_adj_meantemp <- final(m)
#ts_train_adj
length(ts_train_adj_meantemp)
```



```{r}
plot(ts_train_adj_meantemp)
```

Plot original data along with trend and seasonally adjusted data

```{r}
#ts_train
#series(m, "forecast.forecasts")
#out(m)
#seasadj(m)
autoplot(ts_mon_train_meantemp, series="Original Data") +
autolayer(trendcycle(m), series="Trend") +
autolayer(seasadj(m), series="Seasonally Adjusted") +
xlab("Year") + ylab("Mean Temperature") +
ggtitle("Mean Temperature Decomposed using X13") +
scale_colour_manual(values=c("gray","blue","red"),
           breaks=c("Original Data","Seasonally Adjusted","Trend"))
#ap < ggplotly(ap)

```

## **Detrend** {#detrend}

In the seasonally adjusted time series `ts_train_adj`, a trend is salient. It can be removed by differencing as follows:

```{r}
#ts_train_adj_meantemp |> log() |> nsdiffs(alpha=0.01) -> ts_train_adj_det_meantemp
ts_train_adj_meantemp |> log() |> diff() -> ts_train_adj_det_meantemp


```

```{r}
plot(ts_train_adj_det_meantemp)
#plot(d)
```





## **Correlation Plots**

In the following the autocorrelation function (AC) and partial autocorrelation function (PACF) are visualized for both unadjusted and adjusted verions of time series.

1. Weekly aggregated of original time series

```{r}
ggAcf(ts_week_train_meantemp, lag=50)
pacf (ts_week_train_meantemp, lag=50, pl = TRUE)
```

2. Seasonally Adjusted

```{r}
ggAcf(ts_train_adj_meantemp, lag=10)
pacf (ts_train_adj_meantemp, lag=10, pl = TRUE)
```

3. Seasonally Adjusted and Detrended

```{r}
ggAcf(ts_train_adj_det_meantemp, lag=10)
pacf (ts_train_adj_det_meantemp, lag=10, pl = TRUE)
```



## **Prepare Test Set** {#testprepare}

In section [Preprocessing and Analysis Section](#preprocess), all the preprocessing steps are applied on the training dataset. In the following, same processes are applied on the test dataset.

```{r}
summary(df_test)
```

```{r}
df_test |> describe()
```

```{r}
xts_test_meantemp <- xts(df_test$meantemp, order.by=df_test$date, "%Y-%m-%d")
```


```{r}
head(xts_test_meantemp)
```

```{r}
tail(xts_test_meantemp)
```

```{r}
ts_plot(xts_test_meantemp)
```

```{r}
ts_test_meantemp <- ts_ts(xts_test_meantemp)
xts_week_test_meantemp <- apply.weekly(xts_test_meantemp,sum)
ts_week_test_meantemp <- na.remove(ts_ts(xts_week_test_meantemp))
#ts_week_test_meantemp <- as.ts(xts_week_test_meantemp)
```

```{r}
length(ts_week_test_meantemp)
```

```{r}
ts_plot(xts_week_test_meantemp)
```





# **Testing Stationarity: Unit Root Tests** {#stationary}
## ADF

In Augmented Dicky Fuller (ADF) test, the null hypothesis is $H_0$: there is a unit root (equivalently, is a non-stationary time series), while the alternate hypothesis is $H_1$: time series is stationary.
DF test is valid if the time series is well characterized by an $AR(1)$ model with noise errors.
ADF test unlike DF test can be applied on a large sized set of time series models. For this reason, it was preferred in this work over DF test.


1. Original Time Series and its Weekly Adjusted

```{r}
ts_train_meantemp |> adf.test()
```


```{r}
ts_week_train_meantemp |> adf.test()
```

2. Seasonally Adjusted

```{r}
ts_train_adj_meantemp |> adf.test() 
```


3. Seasonally Adjusted and Detrended

```{r}
ts_train_adj_det_meantemp |> adf.test() 
```

In the ADF test, smaller test statistics indicates more likelihood of the null-hypothesis (time series is not stationary) be true. If we set threshold to be 0.05, the p-values less than this value imply null hypothesis is unlikely to be true, and we can increase our certainty in alternate hypothesis (time series is stationary).
For all the investigated datasets, results are reported in the following:

1. Original time series: non-stationary
2. Seasonally adjusted: non-stationary
3. Seasonally adjusted and detrended: stationary
4. Weekly aggregated time series: stationary


## KPSS

In Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, the null hypothesis is $H_0$: time series is stationary (level or trend), while the alternate hypothesis is $H_1$: time series is non-stationary.



1. Original Time Series and also its weekly aggregated 

```{r}
ts_train_meantemp |>  kpss.test()
```
```{r}
ts_week_train_meantemp |>  kpss.test()
```

2. Seasonally Adjusted

```{r}
ts_train_adj_meantemp |>  kpss.test()
```

3. Seasonally Adjusted and Detrended

```{r}
ts_train_adj_det_meantemp |> kpss.test()
```


For all the investigated datasets, results are reported in the following:

1. Original time series: non-stationary
2. Seasonally adjusted: stationary
3. Seasonally adjusted and detrended: non-stationary
4. Weekly aggregated time series: non-stationary



# **Forecasting** {#forecast}

## **SARIMA**

The seasonal autoregressive integrated moving average (SARIMA) is used in the following to forecast time series. The `auto.arima` function is used to find the following parameters, in which `(p, d, q)` correspond to non-seasonal component of time series, whereas `(P, D, Q)` correspond to seasonal component.

- p: Auto-regressive lag order 
- d: Order of first-differencing 
- q: Moving average lag order
- P: Auto-regressive lag order 
- D: Order of seasonal-differencing
- Q: Moving average lag order

The function returns the best ARIMA model according to either AIC.
Since the runtime of the function is very long, the best parameters found are stored, and then they are used as both minimum and maximum option, although the `auto.arima` function nonetheless tries some initial fixed orders before using the predefined lags. Therefore, the options are reduced but not to 1 combination.


1. Forecast original time series of `meantemp`, as the original data has very high frequency, which makes it unsuitable for ARMA.
For this case, I set `seasonal=TRUE`, as in subsequent cases I use data that I seasonally adjusted them already.
Setting `seasonal=TRUE` makes the model more time-consuming. 


```{r include=FALSE, warning=FALSE}
#the parameter `xreg` is used, which is generally for specifying exogenous variables, but it can also be used to pass in a matrix with columns corresponding to the values of p, d, q, P, D and Q.


p = 2
d = 1
q = 3
P = 0
D = 1
Q = 0

# Create a matrix with columns corresponding to p, d, q, P, D and Q
xreg_matrix = cbind(rep(p, length(ts_train_meantemp)), rep(d, length(ts_train_meantemp)), rep(q, length(ts_train_meantemp)),
                 rep(P, length(ts_train_meantemp)), rep(D, length(ts_train_meantemp)), rep(Q, length(ts_train_meantemp)))
```

```{r}
forecast_ts_train_meantemp <- auto.arima(ts_train_meantemp,
                            d = 1,
                            D = 1,
                            start.p = 2,
                            start.q = 3,
                            max.p = 2,
                            max.q = 3,
                            start.P = 0,
                            start.Q = 0,
                            max.P = 0,
                            max.Q = 0,
                            trace = TRUE, 
                            seasonal=TRUE,
                            stepwise=TRUE,
                            approximation=FALSE
                            #,xreg = xreg_matrix
                            )
checkresiduals(forecast_ts_train_meantemp)
```


```{r}
forecast_ts_train_meantemp
```

2. Forecast original time series of meantemp but aggregated weekly, as the original data has very high frequency, which makes it unsuitable for ARMA.
For this case, I set `seasonal=TRUE`, as in case 3, I use data that I seasonally adjusted them already.
Setting `seasonal=TRUE` makes the model more time-consuming. 



```{r}
forecast_ts_week_train_meantemp = auto.arima(ts_week_train_meantemp, 
                                             d = 1, 
                                             D = 1, 
                                             start.p = 4, 
                                             start.q = 0, 
                                             max.p = 4, 
                                             max.q = 0, 
                                             start.P = 1, 
                                             start.Q = 0, 
                                             max.P = 1, 
                                             max.Q = 0, 
                                             trace = TRUE,  
                                             seasonal=TRUE, 
                                             stepwise=FALSE, 
                                             approximation=FALSE)
checkresiduals(forecast_ts_week_train_meantemp)
```



```{r}
forecast_ts_week_train_meantemp
```


3. Forecast deseasonalized time series
```{r}
forecast_ts_train_adj_meantemp = auto.arima(ts_train_adj_meantemp, 
                                            trace = TRUE,  
                                            seasonal= FALSE, 
                                            stepwise=FALSE, 
                                            approximation=FALSE, 
                                            d = 1, 
                                            start.p = 0, 
                                            start.q = 1, 
                                            max.p = 0, 
                                            max.q = 1)
checkresiduals(forecast_ts_train_adj_meantemp)
```

```{r}
forecast_ts_train_adj_meantemp
```





4. Forecast deseasonalized and detrended time series

```{r}
forecast_ts_train_adj_det_meantemp = auto.arima(ts_train_adj_det_meantemp, 
                                                trace = TRUE,
                                                seasonal= FALSE, 
                                                stepwise=FALSE, 
                                                approximation=FALSE,
                                                d = 0, 
                                                start.p = 0, 
                                                start.q = 1, 
                                                max.p = 0, 
                                                max.q = 1)
checkresiduals(forecast_ts_train_adj_det_meantemp)
```






```{r}
#checkresiduals(forecast_ts_train_meantemp)
forecast_ts_train_adj_det_meantemp
```

### Evaluate

Based on the results from the forecast of original data (case 1), we have:

```{r}
AIC_ARMA <- AIC(forecast_ts_train_meantemp)
AIC_ARMA
```

```{r}
BIC_ARMA <- BIC(forecast_ts_train_meantemp)
BIC_ARMA
```

The following evaluation metrics between prediction and test data are manually computed: RMSE, MAE, $R^2$ score.

```{r}
forecast <- forecast_ts_train_meantemp |> forecast(h=114)
#forecast
```


```{r}
predicted <- as.numeric(forecast$mean)
actual <- as.numeric(ts_test_meantemp)

```




```{r}
RMSE_ARMA <- rmse(predicted, actual)
RMSE_ARMA
```


```{r}
MAE_ARMA <- mae(predicted, actual)
MAE_ARMA
```


```{r}
rsq <- function (x, y) cor(x, y) ^ 2

RSQ_ARMA <- rsq(actual, predicted)

RSQ_ARMA
```


Two tables will be presented, one of which reporting metrics of the model applied on training set, and the other reporting metrics for evaluating predictions based on test set.

```{r}

d <- cbind(AIC = AIC_ARMA, BIC = BIC_ARMA)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

```{r}

d <- cbind(R2 = RSQ_ARMA, RMSE = RMSE_ARMA, MAE = MAE_ARMA)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

### Plot Forecast

1. Original time series of meantemp

```{r}
autoplot(forecast(forecast_ts_train_meantemp))# + autolayer(xts_test_meantemp)
```


```{r}
length(ts_test_meantemp)
```

```{r}
forecast_ts_train_meantemp |> forecast(h=114) |>
autoplot() + autolayer(ts_test_meantemp)
```


2. Original time series of meantemp but aggregated weekly


```{r}
#autoplot(forecast(ts_week_train_meantemp)) #+ autolayer(ts_week_test_meantemp)
```


3. Deseasonalized time series


```{r}
#forecast_ts_train_adj + ts_train_adj
autoplot(forecast(forecast_ts_train_adj_meantemp))
```


4. Deseasonalized and detrended time series

```{r}
autoplot(forecast(forecast_ts_train_adj_det_meantemp))
```


The plot of forecasting the test data (using forecast from case 1) joint with test data is displayed in the following:


```{r}
#ts_plot(ts_test_meantemp, forecast$mean)
#ts.union(ts_test_meantemp, forecast$mean)
#forecast$mean

xts_temp <- xts(ts_test_meantemp, order.by=df_test$date, "%Y-%m-%d")
xts_temp_2 <- xts(forecast$mean, order.by=df_test$date, "%Y-%m-%d")
#xts_temp
#xts_temp_2
ts_plot(xts_temp, xts_temp_2)
```



I confronted some issue adding the seasonality and trend componend of the adjusted versions to the original time series. Notwithstanding, judging by the results, the processed original data (without deseaosnalisation and without detrending) is a proper input to the ARMA model, as the function handles seasonalities by itself, and the results demonstrate a promising forecasting performance. Therefore, henceforth the processed time series but with no adjustment are used for the subsequent forecasting parts of this work.

## **Vector autoregressive (VAR)** {#var}

The vector autoregressiv the multivariate time series by using both the columns `meantemp` and `wind_speed`.

The `wind_speed` is plotted interactively in the following:

```{r}
p2 <- df_train |>
  ggplot( aes(x=date, y=wind_speed)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("bitcoin price ($)") +
    theme_ipsum()

# Turn it interactive with ggplotly
p2 <- ggplotly(p2)
#p
p2
```

Same as [Construct Time Series](#initts), a time series object is constructed from the `wind_speed` column.

```{r}
#xts_train_meantemp <- xts(df_train$meantemp, order.by=df_train$date, "%Y-%m-%d")
#ts_train_meantemp <-ts_ts(xts_train_meantemp)

xts_train_windspeed <- xts(df_train$wind_speed, order.by=df_train$date, "%Y-%m-%d")
ts_train_windspeed <-ts_ts(xts_train_windspeed)

```

A static plot of time series is provided below:

```{r}
ts_plot(ts_train_windspeed)
```


The outliers of time series are treated automatically, same as  [Treat Outliers: Automatic](#outlierauto).

```{r}
xts_train_windspeed <- tsclean(xts_train_windspeed)
```


The plot of the resulted time series is visualized below:

```{r}
ts_plot(xts_train_windspeed)
```


The test data for `wind_speed` is created and processed using the same procedure followed for train data:

```{r}
xts_test_windspeed <- xts(df_test$wind_speed, order.by=df_test$date, "%Y-%m-%d")
```


```{r}
head(xts_test_windspeed)
```

```{r}
tail(xts_test_windspeed)
```

```{r}
ts_plot(xts_test_windspeed)
```


```{r}

xts_test_windspeed <- tsclean(xts_test_windspeed)
ts_test_windspeed <- ts_ts(xts_test_windspeed)

```

```{r}
ts_plot(xts_test_windspeed)
```

In what follows, interactive plot of both time series are illustrated:

```{r}
fig <- plot_ly(df_train, type = 'scatter', mode = 'lines')%>%
  add_trace(x = ~date, y = ~meantemp, name = 'MeanTemp')%>%
  add_trace(x = ~date, y = ~wind_speed, name = 'WindSpeed')%>%
  layout(title = 'custom tick labels',legend=list(title=list(text='variable')),
         xaxis = list(dtick = "M1", tickformat= "%b\n%Y"), width = 2000)
options(warn = -1)
fig <- fig %>%
  layout(
         xaxis = list(zerolinecolor = '#ffff',
                      zerolinewidth = 2,
                      gridcolor = 'ffff',  tickangle = 0),
         yaxis = list(zerolinecolor = '#ffff',
                      zerolinewidth = 2,
                      gridcolor = 'ffff'),
         plot_bgcolor='#e5ecf6')


fig
```


Weekly aggregated time series are also constructed:

```{r}
# Weekly mean temperature
xts_week_train_windspeed <- apply.weekly(xts_train_windspeed, sum)
ts_week_train_windspeed <- ts_ts(xts_week_train_windspeed)

xts_week_test_windspeed <- apply.weekly(xts_test_windspeed, sum)
ts_week_test_windspeed <- na.remove(ts_ts(xts_week_test_windspeed))
#ts_week_test_windspeed <- as.ts(xts_week_test_windspeed)


```


```{r}
ts_week_test_meantemp

```

Both original data and weekly aggregated one are visualized in the following:

```{r}
ts_plot(ts_week_train_windspeed)
```

```{r}
ts_plot(xts_train_windspeed)
```

Initially, the original time series is fed to the model, yet it yielded porr performance, which I speculate to be attributed to many fluctuations and seasonality components. On the other hand, using the weekly aggregated data leads to a significant imporve.

Both time sreies are merged and then fed to the VAR model. NA values are also removed from the merged time series.

```{r}
#VAR_data <- ts.union(ts_train_meantemp, ts_train_windspeed)
VAR_data <- ts.union(ts_week_train_meantemp, ts_week_train_windspeed)
colnames(VAR_data) <- cbind("meantemp","wind_speed")
#v1 <- cbind(ts_week_train_meantemp, ts_week_train_windspeed)
#colnames(v1) <- cbind("meantemp","wind_speed")
```

```{r}
#lagselect <- VARselect(v1, type = "both")
#lagselect$selection
```

```{r}
VAR_data <- na.remove(VAR_data)
#tail(v1)
```


We look at different lags suggested by different criteria if we use VAR model.

```{r}
lagselect <- VARselect(VAR_data, season=12, type = "both")
lagselect$selection
```

```{r}
lagselect$criteria
```

Now that we have merged the column `meantemp` with `wind_speed`, we use VAR models with lag to be 10.



```{r}
VAR_est <- VAR(y = VAR_data, season=8, type="both", p=10)
VAR_est

```

```{r}
summary(VAR_est)

```

```{r}
summary(VAR_est$varresult)
```


### Evaluate

Based on the model summary, the value of metrics obtained by the model can be observed, for lag 10:

```{r}
lagselect$criteria[,10]
```


The $R^2$ score of the model after applying can also be reported for both of the time series used:

```{r}
VAR_meantemp_adjr <- summary(VAR_est$varresult$meantemp)$adj.r.squared
VAR_meantemp_adjr
```

```{r}
VAR_windspeed_adjr <- summary(VAR_est$varresult$wind_speed)$adj.r.squared
VAR_windspeed_adjr
```




A Portmanteau test is provided to test that the residuals are uncorrelated. 


```{r}
serial.test(VAR_est, lags.pt=10, type="PT.asymptotic")
```

```{r}
forecasts <- predict(VAR_est, h=114)
```


```{r}
forecast <- VAR_est |> forecast(h=18)
```









Now we use test data to evaluate predictions:

```{r}
predicted_meantemp <- as.numeric(forecast[2]$forecast$meantemp$mean)
actual_meantemp <- as.numeric(ts_week_test_meantemp)

predicted_windspeed <- as.numeric(forecast[2]$forecast$wind_speed$mean)
actual_winspeed <- as.numeric(ts_week_test_windspeed)
```





```{r}
RMSE_meantemp_VAR <- rmse(predicted_meantemp, actual_meantemp)
RMSE_meantemp_VAR
```

```{r}
RMSE_windspeed_VAR <- rmse(predicted_windspeed, actual_winspeed)
RMSE_windspeed_VAR
```


```{r}
MAE_meantemp_VAR <- mae(predicted_meantemp, actual_meantemp)
MAE_meantemp_VAR
```
```{r}
MAE_windspeed_VAR <- mae(predicted_windspeed, actual_winspeed)
MAE_windspeed_VAR
```


```{r}
rsq <- function (x, y) cor(x, y) ^ 2
RSQ_meantemp_VAR <- rsq(predicted_meantemp, actual_meantemp)
RSQ_meantemp_VAR
```

```{r}
RSQ_windspeed_VAR <- rsq(predicted_windspeed, actual_winspeed)
RSQ_windspeed_VAR
```





#### meantemp

```{r}

d <- cbind(Adjusted_R2 = VAR_meantemp_adjr, AIC = lagselect$criteria[,10][0])
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

```{r}

d <- cbind(R2 = RSQ_meantemp_VAR, RMSE = RMSE_meantemp_VAR, MAE = MAE_meantemp_VAR)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```




#### windpseed

```{r}

d <- cbind(Adjusted_R2 = VAR_meantemp_adjr, AIC = lagselect$criteria[,10][0])
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

```{r}

d <- cbind(R2 = RSQ_windspeed_VAR, RMSE = RMSE_windspeed_VAR, MAE = MAE_windspeed_VAR)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```


### Plot Forecast

Firstly, plot of forecasts are displayed, which are based on the model being trained on the training data.

```{r}
plot(forecasts)
```

```{r}
forecast[2]$forecast$meantemp |> autoplot() + autolayer(ts_week_test_meantemp)

```

```{r}
forecast[2]$forecast$wind_speed |>  autoplot() + autolayer(ts_week_test_windspeed)
```

secondly, prediction of test data alongside the actual test data is visualized

```{r}
ts_plot(forecast[2]$forecast$meantemp$mean, ts_week_test_meantemp)
```

```{r}
ts_plot(forecast[2]$forecast$wind_speed$mean, ts_week_test_windspeed)
```


### Granger Causality

```{r}
Granger_meantemp <- causality(VAR_est, cause = "meantemp")
Granger_meantemp
```

```{r}
Granger_windspeed <- causality(VAR_est, cause = "wind_speed")
Granger_windspeed
```


### FEVD

Forecast Error Variance Decomposition (FEVD)

```{r}
FEVD1 <- fevd(VAR_est, n.ahead = 50)
FEVD1
plot(FEVD1)
```



## **Feedforward Neural Network**
### Forecast
#### wind_speed

```{r}
set.seed(34)
# nnetar() requires a numeric vector or time series object as
# input ?nnetar() can be seen for more info on the function
# nnetar() by default fits multiple neural net models and
# gives averaged results xreg option allows for only numeric
# vectors in nnetar() function
fit_windspeed = nnetar(ts_train_windspeed)

```

```{r}
fit_windspeed
```


```{r}
forecast_windspeed <- forecast(fit_windspeed, h = 114, PI = T)
#forecast_windspeed
```





#### meantemp
```{r}
fit_meantemp = nnetar(ts_train_meantemp)
fit_meantemp
```




```{r}
forecast_meantemp <- forecast(fit_meantemp, h = 114, PI = T)
#forecast_meantemp
```



### Evluate

#### meantemp

```{r}
predicted <- as.numeric(forecast_meantemp$mean)
actual <- as.numeric(ts_test_meantemp)
```



```{r}
RMSE_meantemp_NN <- rmse(predicted, actual)
RMSE_meantemp_NN
```


```{r}
MAE_meantemp_NN <- mae(predicted, actual)
MAE_meantemp_NN
```


```{r}
rsq <- function (x, y) cor(x, y) ^ 2

RSQ_meantemp_NN <- rsq(actual, predicted)
RSQ_meantemp_NN
```



#### wind_speed

```{r}
predicted <- as.numeric(forecast_windspeed$mean)
actual <- as.numeric(ts_test_windspeed)
```

```{r}
RMSE_windspeed_NN <- rmse(predicted, actual)
RMSE_windspeed_NN
```


```{r}
MAE_windspeed_NN <- mae(predicted, actual)
MAE_windspeed_NN
```


```{r}
RSQ_windspeed_NN <- rsq(actual, predicted)
RSQ_windspeed_NN
```


Now present tables of report both of time series.

#### meantemp


```{r}

d <- cbind(R2 = RSQ_meantemp_NN, RMSE = RMSE_meantemp_NN, MAE = MAE_meantemp_NN)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```




#### windpseed


```{r}
d <- cbind(R2 = RSQ_windspeed_NN, RMSE = RMSE_windspeed_NN, MAE = MAE_windspeed_NN)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```



### Plot Forecast

First we plot forecasts based on the model being trained on the training data.


```{r}
forecast_windspeed |> autoplot() + autolayer(ts_test_windspeed)
```

```{r}
forecast_meantemp |> autoplot() + autolayer(ts_test_meantemp)
```



Then, we plot the prediction of test data alongside the actual test data.

```{r}
xts_temp <- xts(ts_test_meantemp, order.by=df_test$date, "%Y-%m-%d")
xts_temp_2 <- xts(forecast_meantemp$mean, order.by=df_test$date, "%Y-%m-%d")

ts_plot(xts_temp, xts_temp_2)
```


```{r}
xts_temp <- xts(ts_test_windspeed, order.by=df_test$date, "%Y-%m-%d")
xts_temp_2 <- xts(forecast_windspeed$mean, order.by=df_test$date, "%Y-%m-%d")

ts_plot(xts_temp, xts_temp_2)
```


## **LSTM Neural Network**

The long-short term memory model is used in the following.


```{r}

get_scaling_factors <- function(data){
  out <- c(mean = mean(data), sd = sd(data))
  return(out)
}

normalize_data <- function(data, scaling_factors, reverse = FALSE) {
  
  if (reverse) temp <- (data * scaling_factors[2]) + scaling_factors[1]
  else temp <- (data - scaling_factors[1]) / scaling_factors[2]
  
  out <- temp %>% as.matrix()
  return(out)
}


kerasize_data <- function(data, x = TRUE, lag = 114, pred = 114) {
  
  if (x) {
    
    temp <- sapply(
      1:(length(data) - lag - pred + 1)
      ,function(x) data[x:(x + lag - 1), 1]
    ) %>% t()
    
    out <- array(
      temp %>% unlist() %>% as.numeric()
      ,dim = c(nrow(temp), lag, 1)
    )
    
  }  else {
    
    temp <- sapply(
      (1 + lag):(length(data) - pred + 1)
      ,function(x) data[x:(x + lag - 1), 1]
    ) %>% t()
    
    out <- array(
      temp %>% unlist() %>% as.numeric()
      ,dim = c(nrow(temp), pred, 1)
    )
    
  }
  
  return(out)
  
}

kerasize_pred_input <- function(data, lag = 114, pred = 114){
  temp <- data[(length(data) - pred + 1):length(data)]
  temp <- normalize_data(temp, get_scaling_factors(data))
  out <- array(temp, c(1, lag, 1))
  return(out)
}

```

```{r}
lstm_build_model <- function(x, y, units = 128, batch = 1, epochs = 20, rate = 0.2, seed = 2137){
  
  lag = dim(x)[2]
  
  lstm_model <- keras_model_sequential()

  lstm_model %>%
    layer_lstm(units = 128
               ,batch_input_shape = c(batch, lag, 1)
               ,return_sequences = TRUE
               ,stateful = TRUE) %>%
    layer_dropout(rate = rate) %>%
    layer_lstm(units = 64
               ,return_sequences = TRUE
               ,stateful = TRUE) %>%
    layer_dropout(rate = rate) %>%
    time_distributed(layer_dense(units = 1))

  lstm_model %>%
    compile(loss = 'mean_squared_error'
            ,optimizer = 'adam'
            ,metrics = 'accuracy')

  tensorflow::set_random_seed(seed)
  lstm_model %>% fit(
    x = x
    ,y = y
    ,batch_size = batch
    ,epochs = epochs
    ,verbose = 0
    ,shuffle = FALSE)
  
  out <- list(
    model = lstm_model
    ,x = x
    ,batch = batch
    ,lag = lag
    ,pred = dim(y)[2]
  )
  return(out)

}
```

```{r}
lstm_forecast <- function(x_test, model, scaling_factors){
  
  batch <- model$batch
  
  temp <- model$model %>%
    predict(x_test, batch_size = batch) %>% 
    .[, , 1] %>%
    normalize_data(scaling_factors = scaling_factors, reverse = TRUE)
  
  out <- list(
    forecast = temp
    ,scaling_factors = scaling_factors
  )
  
  return(out)
  
}
```




### Forecast 
#### meantemp

```{r}
# remove the first row of df_test, as it is common with df_train
#df_test <- df_test[-1,] |> head(1)
#data <- as.data.frame(rbind(df_train, df_test))
#data <- merge(df_train, df_test, all=TRUE)

data_meantemp <- ts(c(ts_train_meantemp, ts_test_meantemp), 
           start = start(ts_train_meantemp), 
           frequency = frequency(ts_test_meantemp))	
scaling_factors <- get_scaling_factors(data_meantemp)
data_meantemp_norm <- normalize_data(data_meantemp, scaling_factors)

x_data <- kerasize_data(data_meantemp_norm, x = TRUE, lag = 114, pred = 114)
y_data <- kerasize_data(data_meantemp_norm, x = FALSE, lag = 114, pred = 114)
x_test <- kerasize_pred_input(data_meantemp_norm, lag = 114, pred = 114)

```



```{r}

model <- lstm_build_model(x_data, y_data)
```

```{r}
prediction_meantemp <- lstm_forecast(x_test, model, scaling_factors)
```



#### windspeed

```{r}
data_windspeed <- ts(c(ts_train_windspeed, ts_test_windspeed), 
           start = start(ts_train_windspeed), 
           frequency = frequency(ts_test_windspeed))	
scaling_factors <- get_scaling_factors(data_windspeed)
data_windspeed_norm <- normalize_data(data_windspeed, scaling_factors)

x_data <- kerasize_data(data_windspeed_norm, x = TRUE, lag = 114, pred = 114)
y_data <- kerasize_data(data_windspeed_norm, x = FALSE, lag = 114, pred = 114)
x_test <- kerasize_pred_input(data_windspeed_norm, lag = 114, pred = 114)

```

```{r}
model <- lstm_build_model(x_data, y_data)
```

```{r}
prediction_windspeed <- lstm_forecast(x_test, model, scaling_factors)
```

### Evaluate
#### meantemp

```{r}
predicted_meantemp <- as.numeric(prediction_meantemp$forecast)
actual_meantemp <- as.numeric(ts_test_meantemp)
RMSE_meantemp_LSTM <- rmse(predicted_meantemp, actual_meantemp)
MAE_meantemp_LSTM <- mae(predicted_meantemp, actual_meantemp)
RSQ_meantemp_LSTM <- rsq(predicted_meantemp, actual_meantemp)
```


#### windspeed



```{r}
predicted_windspeed <- as.numeric(prediction_windspeed$forecast)
actual_windspeed <- as.numeric(ts_test_windspeed)
RMSE_windspeed_LSTM <- rmse(predicted_windspeed, actual_windspeed)
MAE_windspeed_LSTM <- mae(predicted_windspeed, actual_windspeed)
RSQ_windspeed_LSTM <- rsq(predicted_windspeed, actual_windspeed)
```


#### Plot Forecast
#### meantemp

```{r}
xts_temp <- xts(ts_test_meantemp, 
                order.by=df_test$date, 
                "%Y-%m-%d")
xts_temp_2 <- xts(prediction_meantemp$forecast, 
                  order.by=df_test$date, 
                  "%Y-%m-%d")

ts_plot(xts_temp, xts_temp_2)
```

#### windspeed

```{r}
xts_temp <- xts(ts_test_windspeed, 
                order.by=df_test$date, 
                "%Y-%m-%d")
xts_temp_2 <- xts(prediction_windspeed$forecast, 
                  order.by=df_test$date, 
                  "%Y-%m-%d")

ts_plot(xts_temp, xts_temp_2)
```



## **Compare Metrics**
```{r}
metrics_list <- c("RMSE", "MAE", "RSQ")
ARMA_meantemp <- c(RMSE_ARMA, MAE_ARMA, RSQ_ARMA)

VAR_meantemp <- c(RMSE_meantemp_VAR, MAE_meantemp_VAR, RSQ_meantemp_VAR)
VAR_windspeed <- c(RMSE_windspeed_VAR, MAE_windspeed_VAR, RSQ_windspeed_VAR)

NN_meantemp <- c(RMSE_meantemp_NN, MAE_meantemp_NN, RSQ_meantemp_NN)
NN_windspeed <- c(RMSE_windspeed_NN, MAE_windspeed_NN, RSQ_windspeed_NN)

LSTM_meantemp <- c(RMSE_meantemp_LSTM,
                   MAE_meantemp_LSTM, 
                   RSQ_meantemp_LSTM)

LSTM_windspeed <- c(RMSE_windspeed_LSTM,
                   MAE_windspeed_LSTM, 
                   RSQ_windspeed_LSTM)

df_eval <- data.frame(metrics_list, ARMA_meantemp, VAR_meantemp, VAR_windspeed, NN_meantemp, NN_windspeed, LSTM_meantemp, LSTM_windspeed)

df_eval[, sapply(df_eval, is.numeric)] <- apply(df_eval[, sapply(df_eval, is.numeric)], 2, round, 3)

colnames(df_eval) <- lapply(colnames(df_eval), function(x) gsub("^.*_", "", x))
```

Note that for the SARMA and Nueral Network models, the original data is used, while weekly aggregated data is used for VAR model.

```{r}
#table(df_eval) |> htmlTable
#knitr::kable(df_eval, col.names = names(df_eval))
#df_eval
htmlTable(df_eval,
          digits = 3,
          cgroup = c("Metrics","ARMA","VAR","NN", "LSTM"),
          n.cgroup = c(1,1,2,2,2),
          
)
```

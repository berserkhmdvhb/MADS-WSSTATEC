---
title: "Daily Climate Time Series Analysis"
author: "Hamed Vaheb"
#1date: "09 Jan 2021"
output:
  html_document:
    number_sections: yes
    toc: yes
    code_folding: hide
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---
# **Import Libraries**
List of used packages:
knitr dplyr ggplot2 broom reshape2 janitor plm pwt9 quarto renv shiny targets testthat tidyverse tibble usethis rio lubridate purrr Hmisc plotly hrbrthemes xts seasonal tsbox forecast tseries plotly ggridges shades urca
```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr) # for knitting markdown files
library(dplyr)
library(ggplot2) # for plotting
library(broom)
library(reshape2)
#library(readr)
#library(readxl)
#library(Ecdat)
library(janitor)
#library(plm)
#library(pwt9)
#library(quarto)
library(renv)
library(shiny)
library(targets)
library(testthat)
library(tidyverse)
library(tibble)
library(lubridate)
library(purrr)
library(Hmisc) # for dependence tests
library(plotly) # for interactive plots
library(hrbrthemes) 
library(xts) # for time series objects
#library(zoo)
library(seasonal) # for seasonality of time series
library(tsbox)
library(forecast) # for forecasting time series
library(tseries) # for unit root tests
#library(seasonalview)
#library(autoplotly)

library(tidyverse) # general
#library(ggalt) # dumbbell plots
library(plotly) #for drawing interactive plots
library(ggridges) #for drawing density gradient
library(shades) #edit colors in natural ways:
library(urca) 
library(tseries)
library(vars) # for VAR models
library(dynlm)
library(Metrics)
library(htmlTable) # for showing tables
#library(keras)
#library(tensorflow)
#install_keras()
#install_tensorflow(version = "nightly")


```

```{r}
library(reticulate)
Sys.unsetenv("RETICULATE_PYTHON") 
use_virtualenv("~/.virtualenvs/r-reticulate")
library(keras)
```

```{r}

colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```
```{r}
#webshot::install_phantomjs()
```


# **Introduction**

## **Describe Dataset**

The dataset used for this project is [Daily Delhi Climate](https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data), which consists of the following columns:
1. `date`: Date of format YYYY-MM-DD starting from "2013-01-01" and ending in "2017-01-01".
<br/>
2. `meantemp`: Mean temperature averaged out from multiple 3 hour intervals in a day.
<br/>
3. `humidity`: Humidity value for the day (units are grams of water vapor per cubic meter volume of air).
<br/>
4. `wind_speed`: Wind speed measured in kmph.
<br/>
5. `mean_pressure`: Pressure reading of weather (measure in atm)

`r colorize("Q1. How can I find out if analyzing meantemp indvidiually (univariate) suffices, or if including other columns and perform a multivariate analysis would worth the effort to find more meaninful patterns and forecasts?", "red")`


## **Goal and Procedure**
The goal of this project is to analyze and forecast the mean temperature Delhi, which is recorded in the `meantemp` column. Three forecasting models, which are autoregressiveâ€“moving-average model (ARMA), Vector autoregression (VAR), and neural network (NN).
After importing the dataset, outliers are removed in [Preprocessing Section](#preprocess) section. Then `meantemp` column is assigned to a time series object in [Construct Time Series](#initts) section for further processing.
After detecting seasonalities using [plots](#seasplots), the time series is [seasonally adjusted](#deseas) using [X13-ARIMA-SEATS](https://cran.r-project.org/package=seasonal/vignettes/seas.pdf) decomposition model. Then, remaining trend is removed in [detrend](#detrend).

Before forecasting the time series, I check for staionarity of time series, as stationarity is an assumption in ARIMA model.
For this purpose, unit root tests are applied in [Stationarity](#stationary) section.

Finally, I used ARIMA model to forecast the time series in [Forecast Time Series](#forecast) section.

```{r}

df_train <- read_csv("data/DailyDelhiClimateTrain.csv")
df_test <- read_csv("data/DailyDelhiClimateTest.csv")
```

```{r}
summary(df_train)
```
```{r}
df_train |> describe()
```

# **Visualize Data** {#viz}
Below we can see interactive plot of the time series.
```{r}
p <- df_train |>
  ggplot( aes(x=date, y=meantemp)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("bitcoin price ($)") +
    theme_ipsum()

# Turn it interactive with ggplotly
p <- ggplotly(p)
#p
p

```

# **Preprocessing** {#preprocess}
We can detect an outlier at the last observation (last row of dataframe). It causes an abrupt decrease in value of temperature. This would lead to problems in further analysis I will proceed and also when I later apply functions on the time series. Therefore, I replace the last observation's value with its previous one.

`r colorize("Q2. Is imputing with the last observation the right approach here? Or is preferrable that I use median of last n (rows) for instance? Maybe it won't matter, as when I aggregate data (per week, month, week, etc.), I remove the last observation.", "red")`
```{r}
previous_value <- df_train$meantemp[df_train$date == as.Date('2016-12-31')]

df_train$meantemp[df_train$date == as.Date('2017-01-01')]<- previous_value 
```

```{r}
#df_train <- head(df_train, -1)
head(df_train)
```

```{r}
tail(df_train)

```
Let us how the plot looks after removing the outlier:

```{r}
p <- df_train |>
  ggplot( aes(x=date, y=meantemp)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("bitcoin price ($)") +
    theme_ipsum()

# Turn it interactive with ggplotly
p <- ggplotly(p)
#p
p

```


Find if there is any missing dates
```{r}
date_range <- seq(min(df_train$date), max(df_train$date), by = 1) 
date_range[!date_range %in% df_train$date] 

```
# **Prepare Test Set** {#testprepare}

```{r}
summary(df_test)
```

```{r}
df_test |> describe()
```

```{r}
xts_test_meantemp <- xts(df_test$meantemp, order.by=df_test$date, "%Y-%m-%d")
```


```{r}
head(xts_test_meantemp)
```

```{r}
tail(xts_test_meantemp)
```

```{r}
ts_plot(xts_test_meantemp)
```
```{r}

ts_test_meantemp <- ts_ts(xts_test_meantemp)
xts_week_test_meantemp <- apply.weekly(xts_test_meantemp,sum)
ts_week_test_meantemp <- na.remove(ts_ts(xts_week_test_meantemp))
#ts_week_test_meantemp <- as.ts(xts_week_test_meantemp)
```

```{r}
length(ts_week_test_meantemp)
```

```{r}
ts_plot(xts_week_test_meantemp)
```


# **Time Series Analysis**
## **Construct Time Series** {#initts}

Now we use the ```meantemp``` column to create our time series data.
I assigned the time series to xts objects. But since many functions later require ts object, each time I define an xts, I also convert it to ts object using ```tsbox::ts_ts``` 


```{r}
min(df_train$date)
max(df_train$date)
```

```{r}
#ts_train <- zoo(df_train$meantemp, df_train$date)

xts_train_meantemp <- xts(df_train$meantemp, order.by=df_train$date, "%Y-%m-%d")
class(xts_train_meantemp)
head(xts_train_meantemp)
tail(xts_train_meantemp)

# convert xts to ts

## Create a daily Date object for ts
#inds <- seq(as.Date("2013-01-01"), as.Date("2017-01-01"), by = "day")

#set.seed(25)
#ts_train <- ts(df_train$meantemp,     # random data
#           start = c(2013, as.numeric(format(inds[1], "%j"))),
#           frequency = 365)


#ts_train <- ts(df_train$meantemp, start = decimal_date(ymd("2013-01-01")), frequency = 365.25 / 7)


```

Convert XTS objects to TS objects:
```{r}
ts_train_meantemp <-ts_ts(xts_train_meantemp)
head(ts_train_meantemp)
tail(ts_train_meantemp)
```

I plot a static plot as well:
```{r}
ts_plot(xts_train_meantemp)
```


## **Seasonality** {#seas}


From the initial plot I judge that there is seasonali. 
For more delicate observation to find if there is more granular periods of seasonality, I use seasonality plots.
Before that, I aggregate data weekly, monthly, and quarterly.

### **Seasonality Plots** {#seasplots}
```{r}
# Weekly mean temperature
xts_week_train_meantemp <- apply.weekly(xts_train_meantemp,sum)
ts_week_train_meantemp <-ts_ts(xts_week_train_meantemp)

# Monthly mean temperature
xts_mon_train_meantemp <- aggregate(xts_train_meantemp, by=as.yearmon, FUN=sum)
ts_mon_train_meantemp <-ts_ts(xts_mon_train_meantemp)

# Quarterly mean temperature
xts_quar_train_meantemp <- aggregate(xts_train_meantemp, as.yearqtr, FUN=sum)
ts_quar_train_meantemp <-ts_ts(xts_quar_train_meantemp)


# Yearly mean temperate
as.year <- function(x) as.integer(as.yearmon(x))
xts_year_train_meantemp <- aggregate(xts_train_meantemp, by=as.year, FUN=sum)
#ts_year_train_meantemp <-ts_ts(xts_year_train_meantemp)
#xts_year_train_meantemp[1]

```


The year 2017 has only one observation, so I remove it from all the aggregated datasets. I couldn't do it before aggregating, otherwise I would have confronted the error ```Error: series has no regular pattern```.

```{r}

xts_week_train_meantemp <- head(xts_week_train_meantemp, -1)
xts_mon_train_meantemp <- head(xts_mon_train_meantemp, -1)
xts_quar_train_meantemp <- head(xts_quar_train_meantemp, -1)

ts_week_train_meantemp <- head(ts_week_train_meantemp, -1)
ts_mon_train_meantemp <- head(ts_mon_train_meantemp, -1)
ts_quar_train_meantemp <- head(ts_quar_train_meantemp, -1)


```


```{r}
#options(repr.plot.width = 7, repr.plot.height =20)
forecast::ggseasonplot(ts_mon_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1) +
  ylab("degree") +
  ggtitle("Seasonal plot: Monthly Mean Temperature")
```
```{r}
forecast::ggseasonplot(ts_mon_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1, polar=TRUE) +
  ylab("degree") +
  ggtitle("Polar Seasonal plot: Monthly Mean Temperature")
```



```{r}
#options(repr.plot.width = 7, repr.plot.height =20)
forecast::ggseasonplot(ts_quar_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1) +
  ylab("degree") +
  ggtitle("Seasonal plot: Quarterly Mean Temperature")
```







```{r}
forecast::ggseasonplot(ts_quar_train_meantemp, year.labels=TRUE, year.labels.left=TRUE, labelgap = 0.1, polar=TRUE) +
  ylab("degree") +
  ggtitle("Polar Seasonal plot: Quarterly Mean Temperature")
```

### **Deseasonalize** {#deseas}

If I need to remove different periods of seasonality together, I would need to use the ```forecast:msts``` function.
For instance in below I remove weekly and yearly seasonality together.
```{r}
des_ts_train_meantemp <- msts(xts_train_meantemp,seasonal.periods = c(7,365))
#head(des_xts_train)
#library(tsbox)
#ts_train <-ts_ts(xts_train)
#ts_train

class(des_ts_train_meantemp)

```
However, since its output had an unfamiliar and weird shape to me, and also since I wasn't sure it uses the state-of-the-art X13 decomposition, I incorporated the [X-13ARIMA-SEATS](http://www.seasonal.website/seasonal.html) using ```seasonal:seas``` function.
However, it has some limitations, as stated in the package's [reference manua](https://www2.census.gov/software/x-13arima-seats/x-13-data/documentation/docx13as.pdf).
For instance, the number of observations must not exceed 780. Nor should maximum seasonal period exceed 12.
That is why I couldn't use original data ```ts_train``` and also the weekly aggregated data ```ts_week_train```, as I would confront the error ```Seasonal period too large```. The only possible aggregated data with highest frequency possible was monthly aggregated, ```ts_mon_train```.
However, I am concerned that I would lose significant pattern and information with this amount of aggregation.

`r colorize("Q3. If you could kindly share your viewpoint here, it would be very helpful for me to ensure how to proceed.", "red")`

```{r}
length(xts_train_meantemp)
length(ts_train_meantemp)


```



```{r}

length(xts_train_meantemp)
nowXTS <-ts_xts(ts_train_meantemp)
length(nowXTS)

length(ts_week_train_meantemp)
```
```{r}
plot(ts_week_train_meantemp)
length(ts_week_train_meantemp)

```

```{r}
plot(ts_train_meantemp)
length(ts_train_meantemp)

```


```{r}
plot(ts_mon_train_meantemp)
length(ts_mon_train_meantemp)
```


```{r}
m <- seas(ts_mon_train_meantemp)
ts_train_adj_meantemp <- final(m)
#ts_train_adj
length(ts_train_adj_meantemp)
```




```{r}
m <- seas(ts_mon_train_meantemp)
ts_train_adj_meantemp <- final(m)
#ts_train_adj
length(ts_train_adj_meantemp)
```



```{r}
plot(ts_train_adj_meantemp)
```

Plot original data along with trend and seasonally adjusted data

```{r}
#ts_train
#series(m, "forecast.forecasts")
#out(m)
#seasadj(m)
autoplot(ts_mon_train_meantemp, series="Original Data") +
autolayer(trendcycle(m), series="Trend") +
autolayer(seasadj(m), series="Seasonally Adjusted") +
xlab("Year") + ylab("Mean Temperature") +
ggtitle("Mean Temperature Decomposed using X13") +
scale_colour_manual(values=c("gray","blue","red"),
           breaks=c("Original Data","Seasonally Adjusted","Trend"))
#ap < ggplotly(ap)

```

## **Detrend** {#detrend}
In the seasonally adjusted time series ```ts_train_adj```, I detected a trend, therefore I detrend it using differencing.

```{r}
#ts_train_adj_meantemp |> log() |> nsdiffs(alpha=0.01) -> ts_train_adj_det_meantemp
ts_train_adj_meantemp |> log() |> diff() -> ts_train_adj_det_meantemp


```

```{r}
plot(ts_train_adj_det_meantemp)
#plot(d)
```





## **Correlation Plots**
1. Weekly aggregated of original time series
```{r}
ggAcf(ts_week_train_meantemp, lag=50)
pacf (ts_week_train_meantemp, lag=50, pl = TRUE)
```

2. Seasonally Adjusted

```{r}
ggAcf(ts_train_adj_meantemp, lag=10)
pacf (ts_train_adj_meantemp, lag=10, pl = TRUE)
```

3. Seasonally Adjusted and Detrended
```{r}
ggAcf(ts_train_adj_det_meantemp, lag=10)
pacf (ts_train_adj_det_meantemp, lag=10, pl = TRUE)
```





## **Testing Stationarity: Unit Root Tests** {#stationary}
### **ADF**

In Augmented Dicky Fuller (ADF) test, the null hypothesis is $H_0:there is a unit root$, while the alternate hypothesis is $H_1: time series is stationary$. 
DF test is valid if the time series is we well characterized by an $AR(1)$ model with noise errors.
ADF test unlike DF test, can applied on a large sized set of time series models.


1. Original Time Series and its Weekly Adjusted
```{r}
ts_train_meantemp |> adf.test()
```


```{r}
ts_week_train_meantemp |> adf.test()
```

2. Seasonally Adjusted

```{r}
ts_train_adj_meantemp |> adf.test() 
```


3. Seasonally Adjusted and Detrended

```{r}
ts_train_adj_det_meantemp |> adf.test() 
```

In all the ADF tests done here, smaller test statistics indicates more likelihood of the null-hypothesis (time series is not stationary) be true. If we set threshold to be 0.05, the p-values less than this value imply null hypothesis is unlikely to be true, and we can increase our certainty in alternate hypothesis.
For all the investigated datasets, results are reported in the following:

1. Original time series: non-stationary
2. Seasonally adjusted: non-stationary
3. Seasonally adjusted and detrended: stationary
4. Weekly aggregated time series: stationary


### **KPSS**

1. Original Time Series and also its weekly aggregated 

```{r}
ts_train_meantemp |>  kpss.test()
```
```{r}
ts_week_train_meantemp |>  kpss.test()
```

2. Seasonally Adjusted

```{r}
ts_train_adj_meantemp |>  kpss.test()
```

3. Seasonally Adjusted and Detrended

```{r}
ts_train_adj_det_meantemp |> kpss.test()
```


For all the investigated datasets, results are reported in the following:

1. Original time series: non-stationary
2. Seasonally adjusted: stationary
3. Seasonally adjusted and detrended: non-stationary
4. Weekly aggregated time series: non-stationary



# **Time Series Forecasting** {#forecast}

## **SARIMA**


1. Forecast original time series of meantemp, as the original data has very high frequency, which makes it unsuitable for ARMA.
For this case, I set `seasonal=TRUE`, as in cases 3m I use data that I seasonally adjusted them already.
Setting `seasonal=TRUE` makes the model more time-consuming. 


```{r}


#forecast_ts_train_meantemp = auto.arima(ts_train_meantemp,
#                            trace = TRUE, 
#                            seasonal=TRUE,
#                            stepwise=FALSE,
#                            approximation=FALSE)
#checkresiduals(forecast_ts_train_meantemp)
```

```{r}
forecast_ts_train_meantemp <- auto.arima(ts_train_meantemp,
                            d = 1,
                            D = 1,
                            start.p = 2,
                            start.q = 3,
                            max.p = 2,
                            #max.d = 1,
                            max.q = 3,
                            start.P = 0,
                            start.Q = 0,
                            max.P = 0,
                            #max.D = 1,
                            max.Q = 0,
                            trace = TRUE, 
                            seasonal=TRUE,
                            stepwise=TRUE,
                            approximation=FALSE
                            #nmodels=
                            )
checkresiduals(forecast_ts_train_meantemp)
```


```{r}
forecast_ts_train_meantemp
```




2. Forecast original time series of meantemp but aggregated weekly, as the original data has very high frequency, which makes it unsuitable for ARMA.
For this case, I set `seasonal=TRUE`, as in case 3, I use data that I seasonally adjusted them already.
Setting `seasonal=TRUE` makes the model more time-consuming. 



```{r}
forecast_ts_week_train_meantemp = auto.arima(ts_week_train_meantemp,
                            trace = TRUE, 
                            seasonal=TRUE,
                            stepwise=FALSE,
                            approximation=FALSE)
checkresiduals(forecast_ts_week_train_meantemp)
```



```{r}
forecast_ts_week_train_meantemp
```


3. Forecast deseasonalized time series
```{r}
forecast_ts_train_adj_meantemp = auto.arima(ts_train_adj_meantemp,
                            trace = TRUE, 
                            seasonal= FALSE,
                            stepwise=FALSE,
                            approximation=FALSE)
checkresiduals(forecast_ts_train_adj_meantemp)
```

```{r}
forecast_ts_train_adj_meantemp
```





4. Forecast deseasonalized and detrended time series
```{r}
forecast_ts_train_adj_det_meantemp = auto.arima(ts_train_adj_det_meantemp,
                            trace = TRUE, 
                            seasonal= FALSE,
                            stepwise=FALSE,
                            approximation=FALSE)
checkresiduals(forecast_ts_train_adj_det_meantemp)
```






```{r}
#checkresiduals(forecast_ts_train_meantemp)
forecast_ts_train_adj_det_meantemp
```

### **Evaluate**

Based on the results from the forecast of original data (case 1), we have:

```{r}
AIC_ARMA <- AIC(forecast_ts_train_meantemp)
AIC_ARMA
```

```{r}
BIC_ARMA <- BIC(forecast_ts_train_meantemp)
BIC_ARMA
```

Now I manually compute the following evaluation metrics between prediction and test data: RMSE, MAE, $R^2$ score.

```{r}
forecast <- forecast_ts_train_meantemp |> forecast(h=114)
#forecast
```


```{r}
predicted <- as.numeric(forecast$mean)
actual <- as.numeric(ts_test_meantemp)

```




```{r}
RMSE_ARMA <- rmse(predicted, actual)
RMSE_ARMA
```


```{r}
MAE_ARMA <- mae(predicted, actual)
MAE_ARMA
```


```{r}
rsq <- function (x, y) cor(x, y) ^ 2

RSQ_ARMA <- rsq(actual, predicted)

RSQ_ARMA
```


Two tables will be presented, one reports metrics of the model applied on training set, and the other reports metrics for evaluating predictions based on test set.

```{r}

d <- cbind(AIC = AIC_ARMA, BIC = BIC_ARMA)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

```{r}

d <- cbind(R2 = RSQ_ARMA, RMSE = RMSE_ARMA, MAE = MAE_ARMA)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

### **Plot Forecast**
1. Original time series of meantemp

```{r}
autoplot(forecast(forecast_ts_train_meantemp))# + autolayer(xts_test_meantemp)
```


```{r}
length(ts_test_meantemp)
```

```{r}
forecast_ts_train_meantemp |> forecast(h=114) |>
autoplot() + autolayer(ts_test_meantemp)
```


2. Original time series of meantemp but aggregated weekly


```{r}
#autoplot(forecast(ts_week_train_meantemp)) #+ autolayer(ts_week_test_meantemp)
```


3. Deseasonalized time series


```{r}
#forecast_ts_train_adj + ts_train_adj
autoplot(forecast(forecast_ts_train_adj_meantemp))
```


4. Deseasonalized and detrended time series

```{r}
autoplot(forecast(forecast_ts_train_adj_det_meantemp))
```


Let us illustrate plot of forecasting the test data (using forecast from case 1) joint with test data.


```{r}
#ts_plot(ts_test_meantemp, forecast$mean)
#ts.union(ts_test_meantemp, forecast$mean)
#forecast$mean

xts_temp <- xts(ts_test_meantemp, order.by=df_test$date, "%Y-%m-%d")
xts_temp_2 <- xts(forecast$mean, order.by=df_test$date, "%Y-%m-%d")
#xts_temp
#xts_temp_2
ts_plot(xts_temp, xts_temp_2)
```

`r colorize("Q4. I did all unit root tests and ARIMA models on all the following datasets: original time series, deseasonalized time series, and combination of deaseonalized and detrended time series. Judging by the plots, the adjusted versions performed poorly when fed to the model compared to feeding the weekly aggregated data that is fed to AUTOARIMA but with autmatic seasonality modelling of seasonality. Could you please share your viewpoint regarding this?", "red")`



## **Vector autoregressive (VAR)**

Now we model a multivariate time series by using both the columns `meantemp` and `wind_speed`.

We plot wind_speed time series first:


We use an interactive plot.
```{r}
p2 <- df_train |>
  ggplot( aes(x=date, y=wind_speed)) +
    geom_area(fill="#69b3a2", alpha=0.5) +
    geom_line(color="#69b3a2") +
    ylab("bitcoin price ($)") +
    theme_ipsum()

# Turn it interactive with ggplotly
p2 <- ggplotly(p2)
#p
p2
```

Now we use a static plot.


```{r}
#xts_train_meantemp <- xts(df_train$meantemp, order.by=df_train$date, "%Y-%m-%d")
#ts_train_meantemp <-ts_ts(xts_train_meantemp)

xts_train_windspeed <- xts(df_train$wind_speed, order.by=df_train$date, "%Y-%m-%d")
ts_train_windspeed <-ts_ts(xts_train_windspeed)

```

```{r}
ts_plot(ts_train_windspeed)
```


We must deal with anomalies first. 



```{r}

xts_train_windspeed <- tsclean(xts_train_windspeed)

```


```{r}
ts_plot(xts_train_windspeed)
```


Let us now do the same with test data of wind_speed column, as we need it later for evaluation:


```{r}
xts_test_windspeed <- xts(df_test$wind_speed, order.by=df_test$date, "%Y-%m-%d")
```


```{r}
head(xts_test_windspeed)
```

```{r}
tail(xts_test_windspeed)
```

```{r}
ts_plot(xts_test_windspeed)
```


We remove anomalies in the test data too:

```{r}

xts_test_windspeed <- tsclean(xts_test_windspeed)
ts_test_windspeed <- ts_ts(xts_test_windspeed)

```

```{r}
ts_plot(xts_test_windspeed)
```

In what follows, interactive plot of both time series are illustrated:

```{r}
fig <- plot_ly(df_train, type = 'scatter', mode = 'lines')%>%
  add_trace(x = ~date, y = ~meantemp, name = 'MeanTemp')%>%
  add_trace(x = ~date, y = ~wind_speed, name = 'WindSpeed')%>%
  layout(title = 'custom tick labels',legend=list(title=list(text='variable')),
         xaxis = list(dtick = "M1", tickformat= "%b\n%Y"), width = 2000)
options(warn = -1)
fig <- fig %>%
  layout(
         xaxis = list(zerolinecolor = '#ffff',
                      zerolinewidth = 2,
                      gridcolor = 'ffff',  tickangle = 0),
         yaxis = list(zerolinecolor = '#ffff',
                      zerolinewidth = 2,
                      gridcolor = 'ffff'),
         plot_bgcolor='#e5ecf6')


fig
```



We aggregate the windspeed time series by weekly
```{r}
# Weekly mean temperature
xts_week_train_windspeed <- apply.weekly(xts_train_windspeed, sum)
ts_week_train_windspeed <- ts_ts(xts_week_train_windspeed)

xts_week_test_windspeed <- apply.weekly(xts_test_windspeed, sum)
ts_week_test_windspeed <- na.remove(ts_ts(xts_week_test_windspeed))
#ts_week_test_windspeed <- as.ts(xts_week_test_windspeed)


```


```{r}
ts_week_test_meantemp

```

Let us plot static plots for them individually as well:

```{r}
ts_plot(ts_week_train_windspeed)
```

```{r}
ts_plot(xts_train_windspeed)
```
I tried the original data on VAR model, but I think due to many fluctuations and seasonality components, it didn't yield accurate results. I fed the weekly aggregated data instead, and I detected a significant imporve.

Now we create a union of both time series and store it.
```{r}
#VAR_data <- ts.union(ts_train_meantemp, ts_train_windspeed)
VAR_data <- ts.union(ts_week_train_meantemp, ts_week_train_windspeed)
colnames(VAR_data) <- cbind("meantemp","wind_speed")
#v1 <- cbind(ts_week_train_meantemp, ts_week_train_windspeed)
#colnames(v1) <- cbind("meantemp","wind_speed")
```

```{r}
#lagselect <- VARselect(v1, type = "both")
#lagselect$selection
```

```{r}
VAR_data <- na.remove(VAR_data)
#tail(v1)
```


We look at different lags suggested by different criteria if we use VAR model.

```{r}
lagselect <- VARselect(VAR_data, season=12, type = "both")
lagselect$selection
```

```{r}
lagselect$criteria
```

Now that we have merged the column `meantemp` with `wind_speed`, we use VAR models with lag to be 10.



```{r}
VAR_est <- VAR(y = VAR_data, season=8, type="both", p=10)
VAR_est

```

```{r}
summary(VAR_est)

```

```{r}
summary(VAR_est$varresult)
```


### **Evaluate**

Based on the model summary, we can look at the value of metrics obtained by the model when we use lag 10:

```{r}
lagselect$criteria[,10]
```


The $R^2$ score of the model after applying can also be reported for both of the time series used:
```{r}
VAR_meantemp_adjr <- summary(VAR_est$varresult$meantemp)$adj.r.squared
VAR_meantemp_adjr
```
```{r}
VAR_windspeed_adjr <- summary(VAR_est$varresult$wind_speed)$adj.r.squared
VAR_windspeed_adjr
```





We test that the residuals are uncorrelated using a Portmanteau test.
```{r}
serial.test(VAR_est, lags.pt=10, type="PT.asymptotic")
```

```{r}
forecasts <- predict(VAR_est, h=114)
```


```{r}
forecast <- VAR_est |> forecast(h=18)
```









Now we use test data to evaluate predictions:

```{r}
predicted_meantemp <- as.numeric(forecast[2]$forecast$meantemp$mean)
actual_meantemp <- as.numeric(ts_week_test_meantemp)

predicted_windspeed <- as.numeric(forecast[2]$forecast$wind_speed$mean)
actual_winspeed <- as.numeric(ts_week_test_windspeed)
```





```{r}
RMSE_meantemp_VAR <- rmse(predicted_meantemp, actual_meantemp)
RMSE_meantemp_VAR
```

```{r}
RMSE_windspeed_VAR <- rmse(predicted_windspeed, actual_winspeed)
RMSE_windspeed_VAR
```


```{r}
MAE_meantemp_VAR <- mae(predicted_meantemp, actual_meantemp)
MAE_meantemp_VAR
```
```{r}
MAE_windspeed_VAR <- mae(predicted_windspeed, actual_winspeed)
MAE_windspeed_VAR
```


```{r}
rsq <- function (x, y) cor(x, y) ^ 2
RSQ_meantemp_VAR <- rsq(predicted_meantemp, actual_meantemp)
RSQ_meantemp_VAR
```

```{r}
RSQ_windspeed_VAR <- rsq(predicted_windspeed, actual_winspeed)
RSQ_windspeed_VAR
```





#### meantemp
```{r}

d <- cbind(Adjusted_R2 = VAR_meantemp_adjr, AIC = lagselect$criteria[,10][0])
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

```{r}

d <- cbind(R2 = RSQ_meantemp_VAR, RMSE = RMSE_meantemp_VAR, MAE = MAE_meantemp_VAR)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```




#### windpseed
```{r}

d <- cbind(Adjusted_R2 = VAR_meantemp_adjr, AIC = lagselect$criteria[,10][0])
# at most 4 decimal places
knitr::kable(d, digits = 4)
```

```{r}

d <- cbind(R2 = RSQ_windspeed_VAR, RMSE = RMSE_windspeed_VAR, MAE = MAE_windspeed_VAR)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```


### **Plot Forecast**

First we plot forecasts based on the model being trained on the training data.
```{r}
plot(forecasts)
```

```{r}
forecast[2]$forecast$meantemp |> autoplot() + autolayer(ts_week_test_meantemp)

```

```{r}
forecast[2]$forecast$wind_speed |>  autoplot() + autolayer(ts_week_test_windspeed)
```

Then, we plot the prediction of test data alongside the actual test data.

```{r}
ts_plot(forecast[2]$forecast$meantemp$mean, ts_week_test_meantemp)
```

```{r}
ts_plot(forecast[2]$forecast$wind_speed$mean, ts_week_test_windspeed)
```


### **Granger Causality**
```{r}
Granger_meantemp <- causality(VAR_est, cause = "meantemp")
Granger_meantemp
```

```{r}
Granger_windspeed <- causality(VAR_est, cause = "wind_speed")
Granger_windspeed
```

### **Forecast Error Variance Decomposition (FEVD)**

```{r}
FEVD1 <- fevd(VAR_est, n.ahead = 50)
FEVD1
plot(FEVD1)
```



## **Feedforward Neural Network**

### **Forecast**
####  wind_speed

```{r}
set.seed(34)
# nnetar() requires a numeric vector or time series object as
# input ?nnetar() can be seen for more info on the function
# nnetar() by default fits multiple neural net models and
# gives averaged results xreg option allows for only numeric
# vectors in nnetar() function
fit_windspeed = nnetar(ts_train_windspeed)

```

```{r}
fit_windspeed
```


```{r}
forecast_windspeed <- forecast(fit_windspeed, h = 114, PI = T)
#forecast_windspeed
```





#### meantemp
```{r}
fit_meantemp = nnetar(ts_train_meantemp)
fit_meantemp
```




```{r}
forecast_meantemp <- forecast(fit_meantemp, h = 114, PI = T)
#forecast_meantemp
```



### **Evluate**

#### meantemp

```{r}
predicted <- as.numeric(forecast_meantemp$mean)
actual <- as.numeric(ts_test_meantemp)
```



```{r}
RMSE_meantemp_NN <- rmse(predicted, actual)
RMSE_meantemp_NN
```


```{r}
MAE_meantemp_NN <- mae(predicted, actual)
MAE_meantemp_NN
```


```{r}
rsq <- function (x, y) cor(x, y) ^ 2

RSQ_meantemp_NN <- rsq(actual, predicted)
RSQ_meantemp_NN
```



#### wind_speed

```{r}
predicted <- as.numeric(forecast_windspeed$mean)
actual <- as.numeric(ts_test_windspeed)
```

```{r}
RMSE_windspeed_NN <- rmse(predicted, actual)
RMSE_windspeed_NN
```


```{r}
MAE_windspeed_NN <- mae(predicted, actual)
MAE_windspeed_NN
```


```{r}
RSQ_windspeed_NN <- rsq(actual, predicted)
RSQ_windspeed_NN
```


Now present tables of report both of time series.
#### meantemp


```{r}

d <- cbind(R2 = RSQ_meantemp_NN, RMSE = RMSE_meantemp_NN, MAE = MAE_meantemp_NN)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```




#### windpseed


```{r}
d <- cbind(R2 = RSQ_windspeed_NN, RMSE = RMSE_windspeed_NN, MAE = MAE_windspeed_NN)
# at most 4 decimal places
knitr::kable(d, digits = 4)
```



### **Forecast Plots**

First we plot forecasts based on the model being trained on the training data.


```{r}
forecast_windspeed |> autoplot() + autolayer(ts_test_windspeed)
```

```{r}
forecast_meantemp |> autoplot() + autolayer(ts_test_meantemp)
```



Then, we plot the prediction of test data alongside the actual test data.

```{r}
xts_temp <- xts(ts_test_meantemp, order.by=df_test$date, "%Y-%m-%d")
xts_temp_2 <- xts(forecast_meantemp$mean, order.by=df_test$date, "%Y-%m-%d")

ts_plot(xts_temp, xts_temp_2)
```


```{r}
xts_temp <- xts(ts_test_windspeed, order.by=df_test$date, "%Y-%m-%d")
xts_temp_2 <- xts(forecast_windspeed$mean, order.by=df_test$date, "%Y-%m-%d")

ts_plot(xts_temp, xts_temp_2)
```


## LSTM Neural Network




```{r}

get_scaling_factors <- function(data){
  out <- c(mean = mean(data), sd = sd(data))
  return(out)
}

normalize_data <- function(data, scaling_factors, reverse = FALSE) {
  
  if (reverse) temp <- (data * scaling_factors[2]) + scaling_factors[1]
  else temp <- (data - scaling_factors[1]) / scaling_factors[2]
  
  out <- temp %>% as.matrix()
  return(out)
}


kerasize_data <- function(data, x = TRUE, lag = 114, pred = 114) {
  
  if (x) {
    
    temp <- sapply(
      1:(length(data) - lag - pred + 1)
      ,function(x) data[x:(x + lag - 1), 1]
    ) %>% t()
    
    out <- array(
      temp %>% unlist() %>% as.numeric()
      ,dim = c(nrow(temp), lag, 1)
    )
    
  }  else {
    
    temp <- sapply(
      (1 + lag):(length(data) - pred + 1)
      ,function(x) data[x:(x + lag - 1), 1]
    ) %>% t()
    
    out <- array(
      temp %>% unlist() %>% as.numeric()
      ,dim = c(nrow(temp), pred, 1)
    )
    
  }
  
  return(out)
  
}

kerasize_pred_input <- function(data, lag = 114, pred = 114){
  temp <- data[(length(data) - pred + 1):length(data)]
  temp <- normalize_data(temp, get_scaling_factors(data))
  out <- array(temp, c(1, lag, 1))
  return(out)
}

```

```{r}
lstm_build_model <- function(x, y, units = 128, batch = 1, epochs = 20, rate = 0.2, seed = 2137){
  
  lag = dim(x)[2]
  
  lstm_model <- keras_model_sequential()

  lstm_model %>%
    layer_lstm(units = 128
               ,batch_input_shape = c(batch, lag, 1)
               ,return_sequences = TRUE
               ,stateful = TRUE) %>%
    layer_dropout(rate = rate) %>%
    layer_lstm(units = 64
               ,return_sequences = TRUE
               ,stateful = TRUE) %>%
    layer_dropout(rate = rate) %>%
    time_distributed(layer_dense(units = 1))

  lstm_model %>%
    compile(loss = 'mean_squared_error'
            ,optimizer = 'adam'
            ,metrics = 'accuracy')

  tensorflow::set_random_seed(seed)
  lstm_model %>% fit(
    x = x
    ,y = y
    ,batch_size = batch
    ,epochs = epochs
    ,verbose = 0
    ,shuffle = FALSE)
  
  out <- list(
    model = lstm_model
    ,x = x
    ,batch = batch
    ,lag = lag
    ,pred = dim(y)[2]
  )
  return(out)

}
```

```{r}
lstm_forecast <- function(x_test, model, scaling_factors){
  
  batch <- model$batch
  
  temp <- model$model %>%
    predict(x_test, batch_size = batch) %>% 
    .[, , 1] %>%
    normalize_data(scaling_factors = scaling_factors, reverse = TRUE)
  
  out <- list(
    forecast = temp
    ,scaling_factors = scaling_factors
  )
  
  return(out)
  
}
```





```{r}
# remove the first row of df_test, as it is common with df_train
#df_test <- df_test[-1,] |> head(1)
#data <- as.data.frame(rbind(df_train, df_test))
#data <- merge(df_train, df_test, all=TRUE)

data_meantemp <- ts(c(ts_train_meantemp, ts_test_meantemp), 
           start = start(ts_train_meantemp), 
           frequency = frequency(ts_test_meantemp))	
scaling_factors <- get_scaling_factors(data_meantemp)
data_meantemp_norm <- normalize_data(data_meantemp, scaling_factors)

x_data <- kerasize_data(data_meantemp_norm, x = TRUE, lag = 114, pred = 114)
y_data <- kerasize_data(data_meantemp_norm, x = FALSE, lag = 114, pred = 114)
x_test <- kerasize_pred_input(data_meantemp_norm, lag = 114, pred = 114)

```

```{r}

model <- lstm_build_model(x_data, y_data)
```

```{r}
prediction <- lstm_forecast(x_test, model, scaling_factors)
```









```{r}
# If whole data was predicted
#temp <- as.data.frame(rbind(df_train,df_test))
#xts_meantemp_pred <- xts(prediction$forecast, #order.by=temp$date, "%Y-%m-%d")
#xts_all <- xts(temp$meantemp, order.by=temp$date, #"%Y-%m-%d")
#ts_plot(xts_meantemp_pred, xts_all)
xts_temp <- xts(ts_test_meantemp, 
                order.by=df_test$date, 
                "%Y-%m-%d")
xts_temp_2 <- xts(prediction$forecast, 
                  order.by=df_test$date, 
                  "%Y-%m-%d")
#xts_temp
#xts_temp_2
ts_plot(xts_temp, xts_temp_2)

```

```{r}
predicted <- as.numeric(prediction$forecast)
actual <- as.numeric(ts_test_meantemp)
RMSE_meantemp_LSTM <- rmse(predicted, actual)
MAE_meantemp_LSTM <- mae(predicted, actual)
RSQ_meantemp_LSTM <- rsq(actual, predicted)
```


```{r}
data_windspeed <- ts(c(ts_train_windspeed, ts_test_windspeed), 
           start = start(ts_train_windspeed), 
           frequency = frequency(ts_test_windspeed))	
scaling_factors <- get_scaling_factors(data_windspeed)
data_windspeed_norm <- normalize_data(data_windspeed, scaling_factors)

x_data <- kerasize_data(data_windspeed_norm, x = TRUE, lag = 114, pred = 114)
y_data <- kerasize_data(data_windspeed_norm, x = FALSE, lag = 114, pred = 114)
x_test <- kerasize_pred_input(data_windspeed_norm, lag = 114, pred = 114)

```

```{r}

model <- lstm_build_model(x_data, y_data)
```

```{r}
prediction <- lstm_forecast(x_test, model, scaling_factors)
```

```{r}
predicted <- as.numeric(prediction$forecast)
actual <- as.numeric(ts_test_windspeed)
RMSE_windspeed_LSTM <- rmse(predicted, actual)
MAE_windspeed_LSTM <- mae(predicted, actual)
RSQ_windspeed_LSTM <- rsq(actual, predicted)
```


## **Compare Metrics**
```{r}
metrics_list <- c("RMSE", "MAE", "RSQ")
ARMA_meantemp <- c(RMSE_ARMA, MAE_ARMA, RSQ_ARMA)

VAR_meantemp <- c(RMSE_meantemp_VAR, MAE_meantemp_VAR, RSQ_meantemp_VAR)
VAR_windspeed <- c(RMSE_windspeed_VAR, MAE_windspeed_VAR, RSQ_windspeed_VAR)

NN_meantemp <- c(RMSE_meantemp_NN, MAE_meantemp_NN, RSQ_meantemp_NN)
NN_windspeed <- c(RMSE_windspeed_NN, MAE_windspeed_NN, RSQ_windspeed_NN)

LSTM_meantemp <- c(RMSE_meantemp_LSTM,
                   MAE_meantemp_LSTM, 
                   RSQ_meantemp_LSTM)

LSTM_windspeed <- c(RMSE_windspeed_LSTM,
                   MAE_windspeed_LSTM, 
                   RSQ_windspeed_LSTM)

df_eval <- data.frame(metrics_list, ARMA_meantemp, VAR_meantemp, VAR_windspeed, NN_meantemp, NN_windspeed, LSTM_meantemp, LSTM_windspeed)
```

Note that for the SARMA and Nueral Network models, the original data is used, while weekly aggregated data is used for VAR model.

```{r}
#table(df_eval) |> htmlTable
#knitr::kable(df_eval, col.names = names(df_eval))
#df_eval
htmlTable(df_eval,
          cgroup = c("Metrics","ARMA","VAR","NN", "LSTM"),
          n.cgroup = c(1,1,2,2,2),
          digits = 3
)
```
